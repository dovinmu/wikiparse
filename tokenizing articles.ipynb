{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import json\n",
    "from wikiparse import indexer, syntax_parser as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening F:/enwiki-20190101-pages-articles-multistream.xml/scratch\\py3\\index.db\n",
      "current mapping 19.1 m pages\n",
      "\n",
      "__init__ complete\n"
     ]
    }
   ],
   "source": [
    "dumps = indexer.load_dumps(build_index=False, scratch_folder='py3')\n",
    "english = dumps['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andre Agassi\n",
      "100 16.5 ms per page\n"
     ]
    }
   ],
   "source": [
    "def tokenize_page(page):\n",
    "    wordfreq = {}\n",
    "    text = page.text\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "    return wordfreq\n",
    "\n",
    "import time\n",
    "tokenize_num = 10\n",
    "ts = time.time()\n",
    "for i in range(tokenize_num):\n",
    "    page = english.get_page_by_index(i)\n",
    "    wordfreq = tokenize_page(page)\n",
    "print(tokenize_num, round(1000*(time.time() - ts)/tokenize_num, 1), 'ms per page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andre Agassi\n"
     ]
    }
   ],
   "source": [
    "df = pandas.Series(wordfreq)\n",
    "print(page.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the              676\n",
       "agassi           480\n",
       "in               438\n",
       "ref              416\n",
       "and              280\n",
       "open             268\n",
       "to               262\n",
       "of               216\n",
       "com              213\n",
       "web              207\n",
       "a                204\n",
       "tennis           200\n",
       "title            195\n",
       "url              194\n",
       "s                189\n",
       "http             180\n",
       "www              174\n",
       "cite             164\n",
       "accessdate       147\n",
       "style            146\n",
       "his              144\n",
       "he               143\n",
       "date             134\n",
       "singles          131\n",
       "andre            131\n",
       "publisher        129\n",
       "at               122\n",
       "background       120\n",
       "us               113\n",
       "men              110\n",
       "                ... \n",
       "masters           45\n",
       "by                44\n",
       "championships     43\n",
       "las               43\n",
       "players           42\n",
       "7                 42\n",
       "time              41\n",
       "2002              41\n",
       "all               40\n",
       "atp               40\n",
       "10                40\n",
       "match             40\n",
       "2001              39\n",
       "federer           39\n",
       "df                38\n",
       "2003              38\n",
       "category          38\n",
       "5                 38\n",
       "sets              37\n",
       "2                 37\n",
       "january           37\n",
       "after             37\n",
       "series            37\n",
       "1995              36\n",
       "mdy               36\n",
       "their             35\n",
       "11                35\n",
       "two               35\n",
       "2010              35\n",
       "3                 34\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Steps\n",
    " - make word count for all of wikipedia, but only include top 100 words from each article\n",
    " - store top 100 words and count from each article"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
